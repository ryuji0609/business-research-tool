import streamlit as st
import pandas as pd
import time
import os
import sys
import importlib.util
from urllib.parse import urlparse

# business_research.py ã‚’ãƒ‘ã‚¹ã«è¿½åŠ ã—ã¦ã‚¤ãƒ³ãƒãƒ¼ãƒˆå¯èƒ½ã«ã™ã‚‹
script_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(script_dir)

try:
    import business_research as core
except ImportError:
    st.error("business_research.py ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚åŒä¸€ãƒ•ã‚©ãƒ«ãƒ€ã«é…ç½®ã—ã¦ãã ã•ã„ã€‚")
    st.stop()

# --- ãƒšãƒ¼ã‚¸è¨­å®š ---
st.set_page_config(
    page_title="ä¼æ¥­ãƒªã‚µãƒ¼ãƒãƒ„ãƒ¼ãƒ« Pro",
    page_icon="ğŸ”",
    layout="wide",
    initial_sidebar_state="expanded",
)

# --- ã‚¹ã‚¿ã‚¤ãƒ« ---
st.markdown("""
    <style>
    .main {
        background-color: #f8f9fa;
    }
    .stButton>button {
        width: 100%;
        border-radius: 5px;
        height: 3em;
        background-color: #007bff;
        color: white;
    }
    .stProgress > div > div > div > div {
        background-color: #007bff;
    }
    </style>
    """, unsafe_allow_html=True)

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼è¨­å®š ---
with st.sidebar:
    st.title("âš™ï¸ è¨­å®š")
    gas_url = st.text_input(
        "Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ (GAS URL)",
        value="https://script.google.com/macros/s/AKfycbzvixEvfoYYuJyx4HrHDQSawutXr37Jm1b54eJ-SNDKa7aT0q6bOsH2UcAwWsqQKSJH/exec",
        help="GASã‚’ãƒ‡ãƒ—ãƒ­ã‚¤ã—ãŸéš›ã®ç™ºè¡ŒURLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚"
    )
    # ã‚¯ãƒ©ã‚¦ãƒ‰ï¼ˆã¾ãŸã¯ãƒ­ãƒ¼ã‚«ãƒ«è¨­å®šï¼‰ã®Secretsã‹ã‚‰ã‚­ãƒ¼ã‚’å®‰å…¨ã«èª­ã¿è¾¼ã‚€
    default_serper = st.secrets.get("SERPER_API_KEY", "") if "SERPER_API_KEY" in st.secrets else ""
    default_openai = st.secrets.get("OPENAI_API_KEY", "") if "OPENAI_API_KEY" in st.secrets else ""

    serper_api_key = st.text_input(
        "Serper APIã‚­ãƒ¼ (æ¤œç´¢ãƒ–ãƒ­ãƒƒã‚¯å›é¿ç”¨)",
        value=default_serper,
        type="password",
        help="1æ—¥ã«ä½•ç™¾ä»¶ã‚‚æ¤œç´¢ã™ã‚‹éš›ã«ã€Googleã‹ã‚‰ã®ãƒ–ãƒ­ãƒƒã‚¯ã‚’å›é¿ã™ã‚‹ãŸã‚ã®APIã‚­ãƒ¼ã§ã™"
    )
    openai_api_key = st.text_input(
        "OpenAI APIã‚­ãƒ¼ (AIã«ã‚ˆã‚‹è¶…é«˜ç²¾åº¦æŠ½å‡ºç”¨)",
        value=default_openai,
        type="password",
        help="ã‚µã‚¤ãƒˆã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰AIãŒä»£è¡¨é€£çµ¡å…ˆã‚’æ­£ç¢ºã«æŠ½å‡ºã™ã‚‹ãŸã‚ã®ã‚­ãƒ¼ã§ã™"
    )
    st.divider()    
    st.info("ã“ã®ãƒ„ãƒ¼ãƒ«ã¯ã€æŒ‡å®šã—ãŸæ¡ä»¶ã§ä¼æ¥­æƒ…å ±ã‚’åé›†ã—ã€CSVä¿å­˜ã¨ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã¸ã®é€ä¿¡ã‚’è¡Œã„ã¾ã™ã€‚")

# --- ãƒ¡ã‚¤ãƒ³ç”»é¢ ---
st.title("ğŸ” ä¼æ¥­ãƒªã‚µãƒ¼ãƒãƒ„ãƒ¼ãƒ« Pro")
st.write("æ¥­ç¨®ã¨åœ°åŸŸã‚’å…¥åŠ›ã—ã¦ã€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆä¼æ¥­ã®é€£çµ¡å…ˆã‚’ç¬æ™‚ã«ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—ã—ã¾ã™ã€‚")

col1, col2, col3 = st.columns([2, 2, 1])

with col1:
    industry = st.text_input("æ¥­ç¨®", placeholder="ä¾‹: ç¾å®¹é™¢, é£²é£Ÿåº—, Webåˆ¶ä½œä¼šç¤¾")
with col2:
    region = st.text_input("åœ°åŸŸ", placeholder="ä¾‹: åŸ¼ç‰çœŒ, æ¸‹è°·åŒº, å¤§é˜ª")
with col3:
    max_count = st.number_input("æœ€å¤§å–å¾—ä»¶æ•°", min_value=1, max_value=500, value=50, step=10)

# urls.txt ã®èª­ã¿è¾¼ã¿ï¼ˆã‚ã‚Œã°ï¼‰
urls_file = os.path.join(script_dir, "urls.txt")
use_urls_txt = False
urls_in_file = []
if os.path.exists(urls_file):
    try:
        with open(urls_file, "r", encoding="utf-8-sig") as f:
            urls_in_file = [line.strip() for line in f if line.strip().startswith("http")]
    except Exception:
        pass

manual_urls_input = ""
if urls_in_file:
    use_urls_txt = st.checkbox(
        f"ğŸ“ ç™»éŒ²æ¸ˆã¿ã® URL ãƒªã‚¹ãƒˆã‚’ä½¿ç”¨ã™ã‚‹ ({len(urls_in_file)} ä»¶)", 
        value=False,
        help="urls.txt ã®ãƒªã‚¹ãƒˆã‚’å„ªå…ˆã—ã¦å‡¦ç†ã—ã¾ã™ã€‚"
    )
    if use_urls_txt:
        st.info("âš ï¸ ç™»éŒ²æ¸ˆã¿ãƒªã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ã§ã™ã€‚ä¸‹ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã¯ç„¡è¦–ã•ã‚Œã¾ã™ã€‚")
else:
    # urls.txt ãŒãªã„ã€ã¾ãŸã¯ç©ºã®å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    with st.expander("ğŸ”— æ‰‹å‹•ã§URLã‚’ç›´æ¥å…¥åŠ›ã™ã‚‹"):
        manual_urls_input = st.text_area(
            "URLã‚’1è¡Œãšã¤è²¼ã‚Šä»˜ã‘ã¦ãã ã•ã„",
            placeholder="https://example.com\nhttps://test.jp",
            help="æ¤œç´¢ãŒã†ã¾ãã„ã‹ãªã„å ´åˆã‚„ã€ç‰¹å®šã®ã‚µã‚¤ãƒˆã ã‘èª¿ã¹ãŸã„æ™‚ã«ä¾¿åˆ©ã§ã™ã€‚"
        )

start_button = st.button("ğŸš€ ãƒªã‚µãƒ¼ãƒé–‹å§‹")

if start_button:
    results = []
    urls = []

    if use_urls_txt:
        urls = urls_in_file
    elif manual_urls_input.strip():
        urls = [u.strip() for u in manual_urls_input.split("\n") if u.strip().startswith("http")]
    
    if not urls and (not industry or not region):
        st.warning("æ¥­ç¨®ã¨åœ°åŸŸã‚’å…¥åŠ›ã™ã‚‹ã‹ã€æ‰‹å‹•ã§URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    else:
        with st.status("ğŸ” èª¿æŸ»ä¸­...", expanded=True) as status:
            # 1. URLåé›†
            if urls:
                st.write(f"âœ… {len(urls)} ä»¶ã®URLã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")
            else:
                query = f"{industry} {region}"
                
                if serper_api_key:
                    st.write(f"âš¡ é«˜é€Ÿæ¤œç´¢APIã‚’ä½¿ç”¨ã—ã¦ {query} ã‚’æ¤œç´¢ä¸­...")
                    urls = core.search_via_api(query, max_count, serper_api_key)
                else:
                    st.write(f"ğŸŒ {query} ã‚’æ¤œç´¢ä¸­... (ãƒ–ãƒ­ãƒƒã‚¯ã•ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚APIè¨­å®šã‚’æ¨å¥¨)")
                    urls = core.search_bing(query, max_count)
                    if len(urls) < 3:
                       st.write("DuckDuckGo ã§è¿½åŠ ã®URLã‚’æ¤œç´¢ä¸­...")
                       ddg = core.search_ddg(query, max_count)
                       seen = {urlparse(u).netloc for u in urls}
                       for u in ddg:
                           if urlparse(u).netloc not in seen:
                               urls.append(u)
                               seen.add(urlparse(u).netloc)
            
            if not urls:
                st.error("URLã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
                if not serper_api_key:
                    st.info("ğŸ’¡ å¯¾ç­–: æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã«ãƒ–ãƒ­ãƒƒã‚¯ã•ã‚Œã¦ã„ã¾ã™ã€‚å·¦å´ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã®ã€ŒSerper APIã‚­ãƒ¼ã€ã‚’è¨­å®šã™ã‚‹ã¨å›é¿ã§ãã¾ã™ã€‚")
                st.stop()
            
            st.write(f"âœ… {len(urls)} ä»¶ã®å¯¾è±¡URLã‚’ç‰¹å®šã—ã¾ã—ãŸã€‚")
            
            progress_bar = st.progress(0)
            data_container = st.empty()
            df_preview = pd.DataFrame()
            
            for i, url in enumerate(urls, 1):
                st.write(f"[{i}/{len(urls)}] {urlparse(url).netloc} ã‚’è§£æä¸­...")
                info = core.scrape_site(url, openai_api_key)
                
                if info["emails"] or info["phones"]:
                    parts = []
                    if info["emails"]: parts.append(f"ãƒ¡ãƒ¼ãƒ«ç­‰å–å¾—")
                    if info["phones"]: parts.append(f"é›»è©±ç•ªå·å–å¾—")
                    st.write(f"  ğŸ‘‰ å–å¾—æˆåŠŸ: {' / '.join(parts)}")
                    
                    results.append(info)
                    
                    # é€”ä¸­çµæœã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼è¡¨ç¤ºï¼ˆæœ‰åŠ¹ãªã‚‚ã®ã®ã¿ï¼‰
                    df_preview = pd.DataFrame([
                        {
                            "æ³•äººå": r["name"],
                            "ãƒ¡ãƒ¼ãƒ«": " / ".join(r["emails"]),
                            "é›»è©±": " / ".join(r["phones"]),
                            "URL": r["url"]
                        } for r in results
                    ])
                    data_container.dataframe(df_preview, use_container_width=True)
                else:
                    st.write("  â†³ âš ï¸ é€£çµ¡å…ˆãŒã²ã¨ã¤ã‚‚è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰")
                
                progress_bar.progress(i / len(urls))
                time.sleep(core.DELAY)
            
            status.update(label="âœ… èª¿æŸ»å®Œäº†ã—ã¾ã—ãŸï¼", state="complete", expanded=False)

        # 3. çµæœã®è¡¨ç¤ºã¨ä¿å­˜
        st.success(f"è¨ˆ {len(results)} ä»¶ã®æƒ…å ±ã‚’å–å¾—ã—ã¾ã—ãŸã€‚")
        
        # CSVä¿å­˜ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œæ™‚ç”¨ï¼‰
        csv_path = core.save_csv(results, industry, region)
        st.info(f"ğŸ’¾ CSVãƒ‡ãƒ¼ã‚¿ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã—ã¾ã—ãŸ: {os.path.basename(csv_path)}")
        
        # Webãƒ–ãƒ©ã‚¦ã‚¶ã‹ã‚‰ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³ï¼ˆSaaSã‚¯ãƒ©ã‚¦ãƒ‰å®Ÿè¡Œæ™‚ç”¨ï¼‰
        csv_data = df_preview.to_csv(index=False).encode("utf-8-sig")
        st.download_button(
            label="â¬‡ï¸ CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            data=csv_data,
            file_name=os.path.basename(csv_path),
            mime="text/csv",
            type="primary"
        )
        
        # ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆé€ä¿¡
        if gas_url:
            with st.spinner("ğŸ“¤ Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«é€ä¿¡ä¸­..."):
                with_info = [r for r in results if r["emails"] or r["phones"]]
                payload = {"results": with_info}
                try:
                    import requests
                    resp = requests.post(gas_url, json=payload, timeout=20)
                    if resp.status_code == 200:
                        st.balloons()
                        st.success("âœ¨ Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã¸è‡ªå‹•é€ä¿¡ã—ã¾ã—ãŸï¼")
                    else:
                        st.error(f"ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆé€ä¿¡ã«å¤±æ•—ã—ã¾ã—ãŸ (HTTP {resp.status_code})")
                except Exception as e:
                    st.error(f"é€ä¿¡æ™‚ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

        # è©³ç´°è¡¨ç¤º
        with st.expander("è©³ç´°ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤º"):
            st.table(df_preview)
